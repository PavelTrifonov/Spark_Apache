{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtWTu0powcs0",
        "outputId": "7902d493-0900-4001-a47d-9722eb217b7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\pavel\\appdata\\roaming\\python\\python311\\site-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\pavel\\appdata\\roaming\\python\\python311\\site-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "#!pip install pyspark >> None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0G23g-BowYAC"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq3pcNrX11GI"
      },
      "source": [
        "### Задание 1: Найти среднее значение элементов в RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0klN5fj4PtS"
      },
      "source": [
        "1. **Создание SparkContext**: Сначала создается объект `SparkContext` с параметрами `\"local\"` и `\"Average RDD\"`. `\"local\"` указывает, что Spark будет работать в локальном режиме, а `\"Average RDD\"` - это имя приложения. `SparkContext` является основным входным точкой для любого Spark-приложения и используется для создания RDD и выполнения операций на кластере.\n",
        "\n",
        "2. **Создание RDD**: Затем используется метод `parallelize` объекта `SparkContext` для создания RDD (Resilient Distributed Dataset) из списка `[1, 2, 3, 4, 5]`. RDD - это основная абстракция в Spark, представляющая собой распределенную коллекцию элементов, которые могут быть обработаны параллельно. Метод `parallelize` копирует элементы коллекции для формирования распределенного набора данных, который может быть обработан параллельно.\n",
        "\n",
        "3. **Вычисление среднего значения**: После создания RDD вызывается метод `mean()` для вычисления среднего значения элементов в RDD.\n",
        "\n",
        "Важно отметить, что количество разделов (partitions) для RDD, созданного с помощью `parallelize`, по умолчанию определяется Spark на основе количества ядер в кластере. Однако, можно указать конкретное количество разделов вторым аргументом метода `parallelize`, если это необходимо для оптимизации производительности."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DM-YBmkw5uI",
        "outputId": "6c1e4f9e-ee0f-42bf-fb45-27abca84a2d1"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage RDD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m mean_value \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mСреднее значение элементов в RDD:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_value)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:2523\u001b[0m, in \u001b[0;36mRDD.mean\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m   2502\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2503\u001b[0m \u001b[38;5;124;03m    Compute the mean of this RDD's elements.\u001b[39;00m\n\u001b[0;32m   2504\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[38;5;124;03m    2.0\u001b[39;00m\n\u001b[0;32m   2522\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:2343\u001b[0m, in \u001b[0;36mRDD.stats\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredFunc\u001b[39m(left_counter: StatCounter, right_counter: StatCounter) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StatCounter:\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m left_counter\u001b[38;5;241m.\u001b[39mmergeStats(right_counter)\n\u001b[1;32m-> 2343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mStatCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mredFunc\u001b[49m\n\u001b[0;32m   2345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[1;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"Average RDD\")\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "mean_value = rdd.mean()\n",
        "print(\"Среднее значение элементов в RDD:\", mean_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFVCxmQzzC0-",
        "outputId": "8c0c3086-7482-4f46-fe3f-be4f2e79d3e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method SparkSession.active of <class 'pyspark.sql.session.SparkSession'>>\n"
          ]
        }
      ],
      "source": [
        "active_session = SparkSession.active\n",
        "print(active_session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h946SGKVyRkF"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laKneGxkxFIy"
      },
      "source": [
        "### Задание 2: найти наибольший элемент в RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU2CqWoZ4ZL3"
      },
      "source": [
        "1. Создает экземпляр `SparkContext` с именем приложения \"Max RDD\" и запускает его на локальном режиме. `SparkContext` является основным объектом в Spark, который позволяет создавать RDD (Resilient Distributed Datasets) и выполнять операции над ними.\n",
        "\n",
        "2. Использует метод `parallelize` для создания RDD из списка чисел `[100, 25, 30, 40, 55, 70]`. Метод `parallelize` позволяет распределить локальную коллекцию данных по рабочим узлам в кластере, преобразуя её в распределённый набор данных (RDD), который может быть обработан параллельно.\n",
        "\n",
        "3. Вызывает метод `max()` на созданном RDD, который возвращает максимальное значение из всех элементов RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8xkx3v6zddv",
        "outputId": "d000670c-6d47-407f-f5de-a34379dc7cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Наибольший элемент в RDD: 100\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"Max RDD\")\n",
        "rdd = sc.parallelize([100, 25, 30, 40, 55, 70])\n",
        "max_value = rdd.max()\n",
        "print(\"Наибольший элемент в RDD:\", max_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvXXvHbzzjlC"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcZF0nMAxqyw"
      },
      "source": [
        "### Задание 3: подсчитать количество элементов, удовлетворяющих определенному условию"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pc66JPw4if5"
      },
      "source": [
        "1. Создание экземпляра `SparkContext` с именем \"Filter RDD\" и запуском в локальном режиме. `SparkContext` является основным входным точкой для любого приложения Spark и обеспечивает доступ к функциональности Spark, таким как создание RDD (Resilient Distributed Datasets) и выполнение операций на данных.\n",
        "\n",
        "2. Создание RDD из списка чисел от 1 до 10 с помощью метода `parallelize`. RDD представляет собой распределенную коллекцию элементов, которые могут быть обработаны параллельно. В данном случае, список `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]` распределяется по кластеру Spark для параллельной обработки.\n",
        "\n",
        "3. Применение фильтрации к RDD с помощью метода `filter`, который принимает функцию-предикат (в данном случае лямбда-функцию) и возвращает новый RDD, содержащий только те элементы, для которых предикат возвращает `True`. В данном случае, фильтрация выполняется для выбора элементов больше 5.\n",
        "\n",
        "4. Подсчет количества элементов в отфильтрованном RDD с помощью метода `count`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpvC5v0iylts",
        "outputId": "fd365ba3-ee1d-42a4-a751-a07338899c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество элементов, больших 5: 5\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"Filter RDD\")\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "filtered_rdd = rdd.filter(lambda x: x > 5)\n",
        "count = filtered_rdd.count()\n",
        "print(\"Количество элементов, больших 5:\", count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlLo54ju1kMM"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwqWm_fCxw8o"
      },
      "source": [
        "### 4. Задание на группировку по ключу:\n",
        "   Дан набор данных с информацией о продажах товаров в магазине в следующем формате: (товар, магазин, количество). Необходимо сгруппировать данные по товару и найти суммарное количество проданных товаров по каждому товару."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TC9sJF74woA"
      },
      "source": [
        "1. **Инициализация SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"advanced\")` создает экземпляр SparkContext, который является точкой входа для любого приложения Spark. В данном случае, SparkContext инициализируется для работы в локальном режиме (`\"local\"`) с именем приложения `\"advanced\"`.\n",
        "\n",
        "2. **Создание списка данных**:\n",
        "   - `data = [(\"apple\", \"store1\", 10), (\"apple\", \"store2\", 15), ...]` определяет список кортежей, где каждый кортеж содержит информацию о фрукте, магазине и количестве.\n",
        "\n",
        "3. **Параллелизация данных**:\n",
        "   - `rdd = sc.parallelize(data)` преобразует список `data` в распределенный набор данных (RDD), который может быть обработан параллельно.\n",
        "\n",
        "4. **Группировка и агрегация данных**:\n",
        "   - `grouped_rdd = rdd.map(lambda x: (x[0], x[2])).reduceByKey(lambda a, b: a + b)` выполняет две операции:\n",
        "     - `map(lambda x: (x[0], x[2]))` преобразует каждый кортеж в RDD, оставляя только первый элемент (название фрукта) и третий элемент (количество) в каждом кортеже.\n",
        "     - `reduceByKey(lambda a, b: a + b)` агрегирует значения по ключу (в данном случае, названию фрукта), суммируя количество для каждого фрукта. Это достигается путем применения функции `lambda a, b: a + b` к значениям, ассоциированным с каждым ключом. Функция `reduceByKey` автоматически выполняет локальную агрегацию на каждом узле перед вычислением глобальных итогов для каждого ключа, что уменьшает объем сетевого трафика.\n",
        "\n",
        "5. **Сбор результатов**:\n",
        "   - `grouped_rdd.collect()` собирает результаты обработки RDD в драйвер программы. В данном случае, это будет список кортежей, где каждый кортеж содержит название фрукта и суммарное количество этого фрукта по всем магазинам.\n",
        "\n",
        "В итоге, код группирует данные по названию фрукта и суммирует количество каждого фрукта по всем магазинам, используя возможности распределенной обработки данных Apache Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "HVqGm0U1ztEB",
        "outputId": "a9bbe480-23e6-4b54-a947-321d6e571ce3"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Max RDD, master=local) created by __init__ at <ipython-input-65-bd938542dd78>:1 ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-63dd3f1f086a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"advanced\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m data = [(\"apple\", \"store1\", 10), (\"apple\", \"store2\", 15),\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"banana\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"store1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"banana\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"store2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"peach\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"store1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"peach\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"store2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         (\"peach\", \"store3\", 25),]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Max RDD, master=local) created by __init__ at <ipython-input-65-bd938542dd78>:1 "
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"advanced\")\n",
        "data = [(\"apple\", \"store1\", 10), (\"apple\", \"store2\", 15),\n",
        "        (\"banana\", \"store1\", 20), (\"banana\", \"store2\", 25),\n",
        "        (\"peach\", \"store1\", 5), (\"peach\", \"store2\", 10),\n",
        "        (\"peach\", \"store3\", 25),]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "grouped_rdd = rdd.map(lambda x: (x[0], x[2])).reduceByKey(lambda a, b: a + b)\n",
        "grouped_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNRkRCV-1LOR"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdNgRly5zzSV"
      },
      "source": [
        "### 5. Задание на агрегацию по ключу:\n",
        "\n",
        "   Дан набор данных с информацией о продажах товаров в магазине в следующем формате: (магазин, товар, количество, цена). Необходимо найти общую выручку от продаж каждого товара в каждом магазине."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UCTUSz2445v"
      },
      "source": [
        "1. **Инициализация SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"advanced\")` создает экземпляр SparkContext, который является точкой входа для любого приложения Spark. В данном случае, SparkContext настроен для работы в локальном режиме (`\"local\"`), что означает, что все вычисления будут выполняться на одной машине. Второй аргумент, `\"advanced\"`, является именем приложения.\n",
        "\n",
        "2. **Создание данных**:\n",
        "   - `data` - это список кортежей, где каждый кортеж содержит информацию о продажах товаров в разных магазинах. Каждый кортеж содержит название магазина, название товара, количество проданных единиц товара и цену за единицу товара.\n",
        "\n",
        "3. **Параллелизация данных**:\n",
        "   - `rdd = sc.parallelize(data)` преобразует список `data` в распределенный набор данных (RDD), который может быть обработан параллельно. Это достигается путем копирования элементов списка в распределенный набор данных, который затем может быть обработан параллельно.\n",
        "\n",
        "4. **Преобразование и агрегация данных**:\n",
        "   - `revenue_rdd = rdd.map(lambda x: ((x[0], x[1]), x[2]*x[3])).reduceByKey(lambda a, b: a + b)` выполняет две операции:\n",
        "     - `map` преобразует каждый элемент RDD, применяя к нему функцию, которая возвращает кортеж, состоящий из пары (магазин, товар) и произведения количества и цены за единицу товара. Это позволяет вычислить общую прибыль от продажи каждого товара в каждом магазине.\n",
        "     - `reduceByKey` агрегирует значения по ключам (в данном случае, по паре (магазин, товар)), суммируя прибыль от продажи каждого товара в каждом магазине. Это делается путем применения функции, которая складывает значения для каждого ключа.\n",
        "\n",
        "5. **Сбор результатов**:\n",
        "   - `revenue_rdd.collect()` собирает результаты обработки данных в машину, на которой запущено приложение. В результате выполнения этого кода будет получен список кортежей, где каждый кортеж содержит пару (магазин, товар) и общую прибыль от продажи этого товара в этом магазине.\n",
        "\n",
        "Важно отметить, что `reduceByKey` автоматически выполняет локальную агрегацию данных на каждом узле перед вычислением глобальных итогов для каждого ключа, что улучшает производительность и снижает объем сетевого трафика."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZHpDf07z2l5",
        "outputId": "c98bedfa-2a4c-4f02-ab7c-a660eec78e18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('store1', 'apple'), 20),\n",
              " (('store2', 'apple'), 37.5),\n",
              " (('store1', 'banana'), 30.0),\n",
              " (('store2', 'banana'), 45.0),\n",
              " (('store1', 'watermelon'), 15),\n",
              " (('store2', 'watermelon'), 8)]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"advanced\")\n",
        "data = [(\"store1\", \"apple\", 10, 2), (\"store2\", \"apple\", 15, 2.5),\n",
        "        (\"store1\", \"banana\", 20, 1.5), (\"store2\", \"banana\", 25, 1.8),\n",
        "        (\"store1\", \"watermelon\", 3, 5), (\"store2\", \"watermelon\", 2, 4)]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "revenue_rdd = rdd.map(lambda x: ((x[0], x[1]), x[2]*x[3])).reduceByKey(lambda a, b: a + b)\n",
        "revenue_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7J2ptTb1W7U"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIr0lkPbz-3f"
      },
      "source": [
        "### 6. Задание на джоин по ключу:\n",
        "\n",
        "   Даны два набора данных: первый с информацией о продажах (товар, количество) и второй с информацией о цене товаров (товар, цена). Необходимо объединить данные и найти общую выручку от продаж каждого товара."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiKP6PJS5B1a"
      },
      "source": [
        "1. **Инициализация SparkContext**:\n",
        "   - Создается экземпляр `SparkContext` с именем \"advanced\", который будет использоваться для выполнения операций в Spark. Параметр \"local\" указывает, что Spark будет работать в локальном режиме, используя все доступные ядра на компьютере.\n",
        "\n",
        "2. **Создание RDD (Resilient Distributed Datasets)**:\n",
        "   - Создаются два списка кортежей: `sales_data` и `price_data`, которые содержат информацию о продажах и ценах на фрукты соответственно.\n",
        "   - Используя метод `parallelize` из `SparkContext`, эти списки преобразуются в RDD. RDD - это распределенный набор данных, который может быть обработан параллельно. Это позволяет Spark эффективно распределять данные по кластеру для параллельной обработки.\n",
        "\n",
        "3. **Объединение RDD**:\n",
        "   - Выполняется операция `join` между `sales_rdd` и `price_rdd`. Этот метод объединяет два RDD по ключам, в данном случае по названиям фруктов. Результатом будет новый RDD, где каждый элемент содержит пару (ключ, (значение из `sales_rdd`, значение из `price_rdd`)).\n",
        "\n",
        "4. **Преобразование данных**:\n",
        "   - Применяется функция `map` к объединенному RDD для вычисления дохода от продаж каждого фрукта. Функция принимает кортеж, где первый элемент - это ключ (название фрукта), а второй элемент - кортеж из двух значений: количество проданных фруктов и цена за единицу. Результатом будет новый RDD, где каждый элемент содержит пару (ключ, доход от продаж).\n",
        "\n",
        "5. **Сбор результатов**:\n",
        "   - Вызывается метод `collect` для получения результатов из RDD в виде списка. Этот метод собирает все элементы RDD в драйвер-программу, что позволяет просмотреть результаты вычислений.\n",
        "\n",
        "В итоге, код вычисляет доходы от продаж для каждого фрукта, используя данные о продажах и ценах, и выводит результаты в виде списка кортежей, где каждый кортеж содержит название фрукта и соответствующий доход."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j8Z3ExW0CJc",
        "outputId": "0e25a345-238f-4f57-bc6a-79da0b5e0181"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('banana', 30.0),\n",
              " ('banana', 37.5),\n",
              " ('peach', 37.5),\n",
              " ('peach', 62.5),\n",
              " ('watermelon', 17.5),\n",
              " ('watermelon', 35.0),\n",
              " ('apple', 20),\n",
              " ('apple', 30)]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"advanced\")\n",
        "sales_data = [(\"apple\", 10), (\"banana\", 20), (\"apple\", 15), (\"banana\", 25), (\"peach\", 15), (\"peach\", 25), (\"watermelon\", 5), (\"watermelon\", 10)]\n",
        "price_data = [(\"apple\", 2), (\"banana\", 1.5), (\"peach\", 2.5), (\"watermelon\", 3.5)]\n",
        "sales_rdd = sc.parallelize(sales_data)\n",
        "price_rdd = sc.parallelize(price_data)\n",
        "joined_rdd = sales_rdd.join(price_rdd)\n",
        "revenue_rdd = joined_rdd.map(lambda x: (x[0], x[1][0] * x[1][1]))\n",
        "revenue_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkXOb2Sl1Xg_"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4UTZ5Qj0H6V"
      },
      "source": [
        "### 7. Поиск самого длинного слова в RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ6Dscr95J2S"
      },
      "source": [
        "Данный код выполняет следующие действия:\n",
        "\n",
        "1. Создает экземпляр `SparkContext` с именем \"RDD tasks\", который используется для выполнения операций с RDD (Resilient Distributed Datasets) в локальном режиме. Этот контекст необходим для работы с Spark и создания RDD из коллекций данных.\n",
        "\n",
        "2. Определяет список строк `data`, содержащий слова на русском языке.\n",
        "\n",
        "3. Использует метод `parallelize` из `SparkContext` для преобразования локальной коллекции `data` в RDD. Это позволяет распределить данные по кластеру для параллельной обработки. В данном случае, поскольку используется локальный режим, данные будут распределены только внутри одного узла.\n",
        "\n",
        "4. Выполняет операцию `max` на RDD, чтобы найти самое длинное слово из списка. Для этого используется лямбда-функция `lambda x: len(x)`, которая возвращает длину каждого слова. Операция `max` сравнивает длины слов и возвращает самое длинное.\n",
        "\n",
        "В итоге, код позволяет определить самое длинное слово из списка, используя возможности распределенной обработки данных, предоставляемые Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWl4FI2U0KY9",
        "outputId": "77179c51-25c2-4c87-d0bd-7e1fc6c24cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Самое длинное слово:  Путеводитель\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"RDD tasks\")\n",
        "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]\n",
        "rdd = sc.parallelize(data)\n",
        "longest_word = rdd.max(key=lambda x: len(x))\n",
        "print(\"Самое длинное слово: \", longest_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQtZzBhr1YNR"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF-kqpmu0PWH"
      },
      "source": [
        "### 8. Фильтрация слов по длине в RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEwyAhaF5Set"
      },
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark с использованием Python API (PySpark):\n",
        "\n",
        "1. **Создание SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"RDD tasks\")`\n",
        "   - Создает экземпляр `SparkContext`, который является основным точкой входа для работы с Spark. В данном случае, контекст настроен для работы в локальном режиме (`\"local\"`), что означает, что все вычисления будут выполняться на одной машине. Второй аргумент, `\"RDD tasks\"`, задает имя приложения, которое будет отображаться в интерфейсе Spark Web UI.\n",
        "\n",
        "2. **Создание списка данных**:\n",
        "   - `data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]`\n",
        "   - Создает список строк, который будет использоваться для создания RDD (Resilient Distributed Datasets).\n",
        "\n",
        "3. **Преобразование списка в RDD**:\n",
        "   - `rdd = sc.parallelize(data)`\n",
        "   - Использует метод `parallelize` из `SparkContext` для преобразования локального списка `data` в RDD. RDD - это распределенная коллекция объектов, которая может быть разделена на несколько партиций для параллельной обработки. В данном случае, `parallelize` автоматически разделяет данные на партиции, распределяя их по узлам в кластере Spark.\n",
        "\n",
        "4. **Фильтрация RDD**:\n",
        "   - `filtered_rdd = rdd.filter(lambda x: len(x) > 6)`\n",
        "   - Применяет функцию фильтрации к RDD, используя лямбда-функцию, которая возвращает `True` для строк длиной более 6 символов. Это приводит к созданию нового RDD, содержащего только те элементы исходного RDD, которые удовлетворяют условию фильтрации.\n",
        "\n",
        "5. **Вывод результатов**:\n",
        "   - `print(\"Слова длиной более 6 символов: \", filtered_rdd.collect())`\n",
        "   - Выводит на экран список слов, длина которых превышает 6 символов. Метод `collect` собирает все элементы RDD обратно в драйвер-программу, что позволяет их вывести на экран.\n",
        "\n",
        "В итоге, код создает RDD из списка слов, фильтрует его, оставляя только слова длиной более 6 символов, и выводит результат на экран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAc8_ldv0QUz",
        "outputId": "68009c6c-4a97-41f8-a5c3-340e0a0ec008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Слова длиной более 6 символов:  ['Приложение', 'Путеводитель', 'Метрополитен']\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"RDD tasks\")\n",
        "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]\n",
        "rdd = sc.parallelize(data)\n",
        "filtered_rdd = rdd.filter(lambda x: len(x) > 6)\n",
        "print(\"Слова длиной более 6 символов: \", filtered_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krNvave71YvD"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tTLqUH60T85"
      },
      "source": [
        "### 9. Подсчет количества уникальных слов в RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlb6yBdJ5bUV"
      },
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark с использованием Python API (PySpark):\n",
        "\n",
        "1. **Создание SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"RDD tasks\")`\n",
        "   - Создает экземпляр `SparkContext`, который является основным точкой входа для работы с Spark. В данном случае, контекст настроен для работы в локальном режиме (`\"local\"`), что означает, что все вычисления будут выполняться на одной машине. Второй аргумент, `\"RDD tasks\"`, задает имя приложения, которое будет отображаться в интерфейсе Spark Web UI.\n",
        "\n",
        "2. **Создание списка данных**:\n",
        "   - `data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]`\n",
        "   - Создает список строк, который будет использоваться для создания RDD (Resilient Distributed Datasets).\n",
        "\n",
        "3. **Преобразование списка в RDD**:\n",
        "   - `rdd = sc.parallelize(data)`\n",
        "   - Использует метод `parallelize` из `SparkContext` для преобразования локального списка `data` в RDD. RDD - это распределенная коллекция объектов, которая может быть разделена на несколько партиций для параллельной обработки. В данном случае, `parallelize` автоматически разделяет данные на партиции, распределяя их по узлам в кластере Spark [0][3].\n",
        "\n",
        "4. **Фильтрация RDD**:\n",
        "   - `filtered_rdd = rdd.filter(lambda x: len(x) > 6)`\n",
        "   - Применяет функцию фильтрации к RDD, используя лямбда-функцию, которая возвращает `True` для строк длиной более 6 символов. Это приводит к созданию нового RDD, содержащего только те элементы исходного RDD, которые удовлетворяют условию фильтрации.\n",
        "\n",
        "5. **Вывод результатов**:\n",
        "   - `print(\"Слова длиной более 6 символов: \", filtered_rdd.collect())`\n",
        "   - Выводит на экран список слов, длина которых превышает 6 символов. Метод `collect` собирает все элементы RDD обратно в драйвер-программу, что позволяет их вывести на экран.\n",
        "\n",
        "В итоге, код создает RDD из списка слов, фильтрует его, оставляя только слова длиной более 6 символов, и выводит результат на экран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCHo2mZa0VJw",
        "outputId": "45b906fd-bc17-4aa7-b444-2b9d65d38c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество уникальных слов  10\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"RDD tasks\")\n",
        "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\",\n",
        "        \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\", \"Яблоко\", \"Путеводитель\", \"Анализ\"]\n",
        "rdd = sc.parallelize(data)\n",
        "unique_words_count = rdd.distinct().count()\n",
        "print(\"Количество уникальных слов \", unique_words_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlog_6RW1ZNV"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMhyLnKN0bRc"
      },
      "source": [
        "### 10. Преобразование всех слов в RDD в верхний регистр"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwV_-H_f5lQI"
      },
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark с использованием PySpark:\n",
        "\n",
        "1. **Инициализация SparkContext**:\n",
        "   - `sc = SparkContext(\"local\", \"RDD tasks\")` создает экземпляр SparkContext, который является основным входным точкой для любого приложения Spark. В данном случае, SparkContext инициализируется для работы в локальном режиме (`\"local\"`), что означает, что все вычисления будут выполняться на одной машине. Второй аргумент, `\"RDD tasks\"`, задает имя приложения, которое будет отображаться в интерфейсе Spark Web UI.\n",
        "\n",
        "2. **Создание списка данных**:\n",
        "   - `data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]` определяет список строк, который будет использоваться в качестве исходных данных.\n",
        "\n",
        "3. **Параллелизация данных**:\n",
        "   - `rdd = sc.parallelize(data)` преобразует список `data` в RDD (Resilient Distributed Dataset), что позволяет Spark распределить данные по нескольким узлам для параллельной обработки. RDD представляет собой неизменяемую, распределенную коллекцию элементов, которые могут быть обработаны параллельно [2].\n",
        "\n",
        "4. **Преобразование данных**:\n",
        "   - `upper_rdd = rdd.map(lambda x: x.upper())` применяет функцию `map` к RDD, которая преобразует каждый элемент списка в верхний регистр. Функция `map` применяется к каждому элементу RDD и возвращает новый RDD, содержащий результаты преобразования. В данном случае, используется лямбда-функция, которая принимает один аргумент `x` и возвращает `x.upper()`, преобразуя каждую строку в верхний регистр [4].\n",
        "\n",
        "5. **Сбор результатов**:\n",
        "   - `print(\"Слова в верхнем регистре \", upper_rdd.collect())` собирает результаты обработки из RDD `upper_rdd` и выводит их. Метод `collect` собирает все элементы RDD обратно на драйвер-узел в виде списка. Это полезно для отладки или когда необходимо получить результаты обработки на драйвер-узеле [0][2].\n",
        "\n",
        "В целом, код демонстрирует базовые операции с RDD в Spark: инициализацию SparkContext, создание RDD из локальной коллекции, применение трансформации к RDD и сбор результатов обработки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpjSNgAF0cF0",
        "outputId": "06345723-4925-4a33-ad0f-61ba401ba174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Слова в верхнем регистре  ['ПРИЛОЖЕНИЕ', 'ЯБЛОКО', 'СПАРК', 'ПУТЕВОДИТЕЛЬ', 'МЕТРОПОЛИТЕН', 'АНАЛИЗ', 'СОЛНЦЕ', 'ПИТОН', 'СНЕГ', 'РЫНОК']\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"RDD tasks\")\n",
        "data = [\"Приложение\", \"Яблоко\", \"Спарк\", \"Путеводитель\", \"Метрополитен\", \"Анализ\", \"Солнце\", \"Питон\", \"Снег\", \"Рынок\"]\n",
        "rdd = sc.parallelize(data)\n",
        "upper_rdd = rdd.map(lambda x: x.upper())\n",
        "print(\"Слова в верхнем регистре \", upper_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlPSZbuN1ZyT"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCBQK13X0g8Z"
      },
      "source": [
        "### 11. Найти средний возраст пользователей по их покупкам и вывести топ-5 самых молодых пользователей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1CLQW2h5vRy"
      },
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark для обработки данных о возрасте пользователей и их покупках:\n",
        "\n",
        "1. **Инициализация SparkContext**:\n",
        "   ```python\n",
        "   sc = SparkContext(\"local\", \"UserAge\")\n",
        "   ```\n",
        "   Создается экземпляр `SparkContext`, который является точкой входа для любого приложения Spark. В данном случае, приложение запускается локально, и ему присваивается имя \"UserAge\".\n",
        "\n",
        "2. **Создание RDD**:\n",
        "   ```python\n",
        "   user_purchase_rdd = sc.parallelize([(1, 25), (2, 30), (3, 20), (4, 35), (5, 28), (6, 22)])\n",
        "   ```\n",
        "   Создается RDD (Resilient Distributed Dataset) с именем `user_purchase_rdd`, содержащий пары (user_id, age), где `user_id` - идентификатор пользователя, а `age` - его возраст. Метод `parallelize` используется для создания RDD из списка пар.\n",
        "\n",
        "3. **Преобразование и агрегация данных**:\n",
        "   ```python\n",
        "   user_age_total = user_purchase_rdd.mapValues(lambda age: (age, 1)).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
        "   ```\n",
        "   - `mapValues(lambda age: (age, 1))` преобразует каждую пару (user_id, age) в пару (user_id, (age, 1)), где второй элемент является кортежем, содержащим возраст и счетчик единиц.\n",
        "   - `reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))` агрегирует значения по ключам (user_id), суммируя возраст и счетчик единиц для каждого пользователя.\n",
        "   - `mapValues(lambda v: v[0] / v[1])` вычисляет средний возраст для каждого пользователя, разделив сумму возрастов на количество записей.\n",
        "\n",
        "4. **Сортировка и выборка**:\n",
        "   ```python\n",
        "   youngest_users = user_age_total.sortBy(lambda x: x[1]).take(5)\n",
        "   ```\n",
        "   - `sortBy(lambda x: x[1])` сортирует пары (user_id, avg_age) по возрастанию среднего возраста.\n",
        "   - `take(5)` выбирает первые 5 пользователей по возрастанию среднего возраста.\n",
        "\n",
        "5. **Вывод результатов**:\n",
        "   ```python\n",
        "   for user_id, avg_age in youngest_users:\n",
        "       print(user_id, avg_age)\n",
        "   ```\n",
        "   Выводит идентификаторы и средние возрасты пяти самых молодых пользователей.\n",
        "\n",
        "В целом, код выполняет агрегацию данных о возрасте пользователей, вычисляет средний возраст для каждого пользователя и выводит идентификаторы и средние возрасты пяти самых молодых пользователей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvJH38Cd0h4A",
        "outputId": "79267c8f-870f-4df6-9192-8460e5a6bc9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 20.0\n",
            "6 22.0\n",
            "1 25.0\n",
            "5 28.0\n",
            "2 30.0\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"UserAge\")\n",
        "user_purchase_rdd = sc.parallelize([(1, 25), (2, 30), (3, 20), (4, 35), (5, 28), (6, 22)])\n",
        "user_age_total = user_purchase_rdd.mapValues(lambda age: (age, 1)).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
        "youngest_users = user_age_total.sortBy(lambda x: x[1]).take(5)\n",
        "for user_id, avg_age in youngest_users:\n",
        "\tprint(user_id, avg_age)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dboHrId1aUv"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhM3A6aJ0nkI"
      },
      "source": [
        "### 12. Найти среднюю цену товара в каждой категории и вывести результат в формате \"Категория: Средняя цена\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSFZ_cz53wA"
      },
      "source": [
        "Данный код выполняет следующие действия:\n",
        "\n",
        "1. Создает экземпляр `SparkContext` с именем \"ProductPrice\", который используется для взаимодействия с кластером Spark. Параметр \"local\" указывает, что Spark будет работать в локальном режиме, а не на кластере.\n",
        "\n",
        "2. Создает RDD (Resilient Distributed Dataset) с именем `product_rdd`, используя метод `parallelize`. Этот RDD содержит кортежи, где каждый кортеж представляет собой тройку: идентификатор продукта, категорию продукта и цену продукта.\n",
        "\n",
        "3. Применяет функцию `map` к `product_rdd`, чтобы преобразовать каждый кортеж в пару ключ-значение, где ключом является категория продукта, а значением — кортеж, содержащий цену и количество единиц продукта (в данном случае всегда 1).\n",
        "\n",
        "4. Использует `reduceByKey` для агрегации значений по ключу (категории продукта). Функция `reduceByKey` принимает функцию, которая определяет, как агрегировать значения. В данном случае, функция суммирует цены и количество единиц продукта для каждой категории.\n",
        "\n",
        "5. После агрегации, `mapValues` преобразует значения в RDD, вычисляя среднюю цену для каждой категории, разделив общую сумму цен на количество единиц продукта.\n",
        "\n",
        "6. Наконец, `collect` собирает результаты в драйвер программы, и цикл `for` выводит каждую категорию и ее среднюю цену.\n",
        "\n",
        "В итоге, код вычисляет среднюю цену продуктов по категориям, используя функции `map`, `reduceByKey`, и `mapValues` для обработки и агрегации данных в RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqCBO7Dm0obi",
        "outputId": "f42b3864-f422-4709-a73a-2a4442dbb6d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A 110.0\n",
            "B 140.0\n",
            "C 170.0\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"ProductPrice\")\n",
        "product_rdd = sc.parallelize([(1, \"A\", 100), (2, \"B\", 150), (3, \"A\", 120), (4, \"C\", 200), (5, \"B\", 130), (6, \"C\", 140)])\n",
        "category_total_price = product_rdd.map(lambda x: (x[1], (x[2], 1))).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
        "for category, avg_price in category_total_price.collect():\n",
        "\t  print(category, avg_price)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIRMo2rI1azG"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxxMQ7BI0wxy"
      },
      "source": [
        "### 13. Найти все пары чисел из RDD, сумма которых превышает 100, и вывести их."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0EDTNVs6Cx8"
      },
      "source": [
        "Данный код выполняет следующие действия в контексте Apache Spark:\n",
        "\n",
        "1. **Инициализация SparkContext**: Сначала создается экземпляр `SparkContext` с именем \"NumberPairs\", который будет использоваться для выполнения операций в Spark. Параметр \"local\" указывает, что Spark будет работать в локальном режиме, а не на кластере [4].\n",
        "\n",
        "2. **Создание RDD**: Затем создается RDD (Resilient Distributed Dataset) с именем `number_rdd`, используя метод `parallelize` на `SparkContext`. Этот метод преобразует обычный список чисел в распределенный набор данных, который может быть обработан параллельно. В данном случае, список содержит числа от 30 до 140 [0].\n",
        "\n",
        "3. **Операция Cartesian Product**: Далее, на `number_rdd` применяется операция `cartesian`, которая создает декартово произведение RDD с самим собой. Это означает, что для каждого элемента в `number_rdd` будет создана пара, где первый элемент пары - это исходный элемент, а второй - любой другой элемент из `number_rdd`. В результате получается RDD, содержащий все возможные пары чисел из исходного списка [2].\n",
        "\n",
        "4. **Фильтрация пар**: После создания декартового произведения, к RDD применяется фильтрация с помощью метода `filter`. Фильтр сохраняет только те пары чисел, сумма которых больше 100. Это достигается с помощью лямбда-функции, которая принимает пару чисел и возвращает `True`, если сумма чисел в паре больше 100, и `False` в противном случае [2].\n",
        "\n",
        "5. **Сборка результатов**: Наконец, метод `collect` используется для сбора всех оставшихся после фильтрации пар чисел в массив на драйвере. Этот массив затем перебирается в цикле `for`, и каждая пара чисел выводится на экран [0].\n",
        "\n",
        "В итоге, код генерирует и выводит все пары чисел из исходного списка, сумма которых больше 100, используя возможности распределенных вычислений Apache Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGYAuo7M0x3s",
        "outputId": "4ecaaf36-6c39-4d82-af52-a1ce02e551d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(30, 80)\n",
            "(30, 90)\n",
            "(30, 100)\n",
            "(30, 110)\n",
            "(30, 120)\n",
            "(30, 130)\n",
            "(30, 140)\n",
            "(40, 70)\n",
            "(40, 80)\n",
            "(40, 90)\n",
            "(40, 100)\n",
            "(40, 110)\n",
            "(40, 120)\n",
            "(40, 130)\n",
            "(40, 140)\n",
            "(50, 60)\n",
            "(50, 70)\n",
            "(50, 80)\n",
            "(50, 90)\n",
            "(50, 100)\n",
            "(50, 110)\n",
            "(50, 120)\n",
            "(50, 130)\n",
            "(50, 140)\n",
            "(60, 50)\n",
            "(60, 60)\n",
            "(60, 70)\n",
            "(60, 80)\n",
            "(60, 90)\n",
            "(60, 100)\n",
            "(60, 110)\n",
            "(60, 120)\n",
            "(60, 130)\n",
            "(60, 140)\n",
            "(70, 40)\n",
            "(70, 50)\n",
            "(70, 60)\n",
            "(70, 70)\n",
            "(70, 80)\n",
            "(70, 90)\n",
            "(70, 100)\n",
            "(70, 110)\n",
            "(70, 120)\n",
            "(70, 130)\n",
            "(70, 140)\n",
            "(80, 30)\n",
            "(80, 40)\n",
            "(80, 50)\n",
            "(80, 60)\n",
            "(80, 70)\n",
            "(80, 80)\n",
            "(80, 90)\n",
            "(80, 100)\n",
            "(80, 110)\n",
            "(80, 120)\n",
            "(80, 130)\n",
            "(80, 140)\n",
            "(90, 30)\n",
            "(90, 40)\n",
            "(90, 50)\n",
            "(90, 60)\n",
            "(90, 70)\n",
            "(90, 80)\n",
            "(90, 90)\n",
            "(90, 100)\n",
            "(90, 110)\n",
            "(90, 120)\n",
            "(90, 130)\n",
            "(90, 140)\n",
            "(100, 30)\n",
            "(100, 40)\n",
            "(100, 50)\n",
            "(100, 60)\n",
            "(100, 70)\n",
            "(100, 80)\n",
            "(100, 90)\n",
            "(100, 100)\n",
            "(100, 110)\n",
            "(100, 120)\n",
            "(100, 130)\n",
            "(100, 140)\n",
            "(110, 30)\n",
            "(110, 40)\n",
            "(110, 50)\n",
            "(110, 60)\n",
            "(110, 70)\n",
            "(110, 80)\n",
            "(110, 90)\n",
            "(110, 100)\n",
            "(110, 110)\n",
            "(110, 120)\n",
            "(110, 130)\n",
            "(110, 140)\n",
            "(120, 30)\n",
            "(120, 40)\n",
            "(120, 50)\n",
            "(120, 60)\n",
            "(120, 70)\n",
            "(120, 80)\n",
            "(120, 90)\n",
            "(120, 100)\n",
            "(120, 110)\n",
            "(120, 120)\n",
            "(120, 130)\n",
            "(120, 140)\n",
            "(130, 30)\n",
            "(130, 40)\n",
            "(130, 50)\n",
            "(130, 60)\n",
            "(130, 70)\n",
            "(130, 80)\n",
            "(130, 90)\n",
            "(130, 100)\n",
            "(130, 110)\n",
            "(130, 120)\n",
            "(130, 130)\n",
            "(130, 140)\n",
            "(140, 30)\n",
            "(140, 40)\n",
            "(140, 50)\n",
            "(140, 60)\n",
            "(140, 70)\n",
            "(140, 80)\n",
            "(140, 90)\n",
            "(140, 100)\n",
            "(140, 110)\n",
            "(140, 120)\n",
            "(140, 130)\n",
            "(140, 140)\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext(\"local\", \"NumberPairs\")\n",
        "number_rdd = sc.parallelize([30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140])\n",
        "number_pairs = number_rdd.cartesian(number_rdd).filter(lambda x: x[0] + x[1] > 100).collect()\n",
        "for pair in number_pairs:\n",
        "    print(pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lXAOBptO1bZB"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7_jUYja01Zv"
      },
      "source": [
        "Домашнее задание:\n",
        "Условие: Найти самую длинную последовательность упорядоченных чисел в RDD и вывести её"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc = SparkContext(\"local\", 'Max_len')\n",
        "rdd = sc.parallelize([1, 1, 2, 3, 5, 7, 12, 13, 14, 15, 15, 25, 31])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iterator = rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "[1, 2]\n",
            "[]\n",
            "[]\n",
            "[12, 13, 14]\n",
            "[]\n",
            "[]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(4, [12, 13, 14, 15])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def search_max_len(iter):\n",
        "    max_len = 0\n",
        "    numb_list = []\n",
        "    index_max_len = 0\n",
        "    index_numb = 0\n",
        "    while index_numb < len(iter) - 1:\n",
        "        if iter[index_numb]+1 == (iter[index_numb + 1]):\n",
        "            numb_list.append(iter[index_numb])\n",
        "            index_numb += 1\n",
        "        else:\n",
        "            if max_len < len(numb_list) + 1:\n",
        "                max_len = len(numb_list) + 1\n",
        "                index_max_len = index_numb \n",
        "            print(numb_list)    \n",
        "            numb_list = []\n",
        "            index_numb += 1\n",
        "    return max_len, iter[index_max_len-max_len+1: index_max_len+1]\n",
        "\n",
        "search_max_len([1, 1, 2, 3, 5, 7, 12, 13, 14, 15, 15, 25, 31])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_max_len(index, iter):\n",
        "    max_len = 0\n",
        "    numb_list = []\n",
        "    index_max_len = 0\n",
        "    while index < len(iter) - 1:\n",
        "        if iter[index] + 1 == (iter[index + 1]):\n",
        "            numb_list.append(iter[index])\n",
        "            index += 1\n",
        "        else:\n",
        "            if max_len < len(numb_list) + 1:\n",
        "                max_len = len(numb_list) + 1\n",
        "                index_max_len = index\n",
        "            print(numb_list)    \n",
        "            numb_list = []\n",
        "            index += 1\n",
        "    return max_len, iter[index_max_len-max_len+1: index_max_len+1]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Lq3pcNrX11GI",
        "laKneGxkxFIy",
        "dcZF0nMAxqyw",
        "gwqWm_fCxw8o",
        "XdNgRly5zzSV",
        "XIr0lkPbz-3f",
        "V4UTZ5Qj0H6V",
        "jF-kqpmu0PWH",
        "0tTLqUH60T85",
        "qMhyLnKN0bRc",
        "CCBQK13X0g8Z",
        "EhM3A6aJ0nkI",
        "AxxMQ7BI0wxy"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
